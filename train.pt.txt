import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator


def data_generator(X, y, batch_size):
    steps_per_epoch = len(X) // batch_size
    while True:
        for i in range(steps_per_epoch):
            X_batch = X[i * batch_size : (i + 1) * batch_size]
            y_batch = y[i * batch_size : (i + 1) * batch_size]
            yield X_batch, y_batch


chunk_size = 1000 
df_chunks = pd.read_csv('/content/drive/MyDrive/ML/RF_Data_freq.csv', chunksize=chunk_size)


X_data = []
y_data = []


for chunk in df_chunks:

    X_chunk = chunk.iloc[:, 1:].values  
    y_chunk = chunk.iloc[:, 0].values   

  
    X_data.append(X_chunk)
    y_data.append(y_chunk)


X_data = np.concatenate(X_data, axis=0)
y_data = np.concatenate(y_data, axis=0)


X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)


input_shape = (X_train.shape[1], 1)  
X_train = X_train.reshape(-1, *input_shape)
X_test = X_test.reshape(-1, *input_shape)


model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


early_stop = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)
model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/nandu/best_model.h5', save_best_only=True)


batch_size = 16  
train_generator = data_generator(X_train, y_train, batch_size)
steps_per_epoch = len(X_train) // batch_size


model.fit(train_generator, validation_data=(X_test, y_test), epochs=10, steps_per_epoch=steps_per_epoch,
          callbacks=[early_stop, model_checkpoint])


model.load_weights('/content/drive/MyDrive/nandu/best_model.h5')


loss, accuracy = model.evaluate(X_test, y_test)
print("Testing Loss:", loss)
print("Testing Accuracy:", accuracy)


plt.title('Training and Validation Loss')

target_column_index = data.columns.get_loc('0.0005604')


X = data.drop(data.columns[target_column_index], axis=1).values
y = data.iloc[:, target_column_index].values
print(target_column_index)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader


class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out

class CustomDataset(Dataset):
    def __init__(self, csv_file):
        self.data = pd.read_csv(csv_file)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx].values
        return sample


torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size = 1
hidden_size = 64
output_size = 1
num_epochs = 100
batch_size = 32
learning_rate = 0.001

dataset = CustomDataset('RF_Data_freq.csv')
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
model = RNN(input_size, hidden_size, output_size).to(device)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


for epoch in range(num_epochs):
    total_loss = 0.0

    for batch_data in data_loader:
        inputs = batch_data[:, :-1].unsqueeze(2).float().to(device)
        labels = batch_data[:, -1].unsqueeze(1).float().to(device)

        
        outputs = model(inputs)
        loss = criterion(outputs, labels)

       
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(data_loader):.4f}")


torch.save(model.state_dict(), 'trained_model.pt')

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out
class CustomDataset(Dataset):
    def __init__(self, csv_file):
        self.data = pd.read_csv(csv_file)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx].values
        return sample


torch.manual_seed(0)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

input_size = 1
hidden_size = 64
output_size = 1
num_epochs = 30
batch_size = 32
learning_rate = 0.001


dataset = CustomDataset('RF_Data_freq.csv')
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


model = RNN(input_size, hidden_size, output_size).to(device)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


for epoch in range(num_epochs):
    total_loss = 0.0
    total_efficiency = 0.0

    for batch_data in data_loader:
        inputs = batch_data[:, :-1].unsqueeze(2).float().to(device)
        labels = batch_data[:, -1].unsqueeze(1).float().to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        
        predicted_labels = outputs.detach().cpu().numpy()
        true_labels = labels.detach().cpu().numpy()
        efficiency = np.mean(np.abs(predicted_labels - true_labels) / true_labels) * 100
        total_efficiency += efficiency

        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(data_loader)
    avg_efficiency = total_efficiency / len(data_loader)
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Efficiency: {avg_efficiency:.2f}%")
