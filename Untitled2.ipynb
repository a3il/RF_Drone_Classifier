{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2wiTiQ/IrSRLZSUMHXjLn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"pBUk1qOXQSXV","executionInfo":{"status":"error","timestamp":1714149352882,"user_tz":-330,"elapsed":12390,"user":{"displayName":"Adil Ansari","userId":"01083485150708236568"}},"outputId":"7ef8d6f8-4848-43b4-9e5f-33f1b7711841"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]},{"output_type":"error","ename":"SystemError","evalue":"GPU device not found","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-30b17e1c2041>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemError\u001b[0m: GPU device not found"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import (\n","    accuracy_score, confusion_matrix, classification_report,\n","    f1_score, precision_score, recall_score\n",")\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from imblearn.over_sampling import RandomOverSampler\n","import ast\n","from tqdm import tqdm\n","from joblib import parallel_backend\n","# Enable GPU and CUDA acceleration\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","!pip install -q nvidia-cudnn-cu11\n","\n","# Rest of your code here\n","# Function to parse string representation of numpy array\n","def parse_numpy_array(s):\n","    s = s.replace('e', 'E').replace('\\n', ' ')\n","    s = ''.join(filter(lambda x: x.isdigit() or x in '. ,', s))\n","    s = s.replace(',', ' ').replace('  ', ' ')\n","    return np.fromstring(s, sep=' ', dtype=float)\n","\n","# Function to load data from CSV file\n","def load_data(csv_file):\n","    data = pd.read_csv(csv_file)\n","\n","    data['slic_features'] = data['slic_features'].apply(lambda x: ast.literal_eval(x))\n","    data['lbp_features'] = data['lbp_features'].apply(parse_numpy_array)\n","    data['histogram_features'] = data['histogram_features'].apply(parse_numpy_array)\n","\n","    label_encoder = LabelEncoder()\n","    data['class_encoded'] = label_encoder.fit_transform(data['class'])\n","\n","    return data, label_encoder\n","\n","# Function to plot confusion matrix\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()\n","\n","# Function to train and evaluate SVM classifier\n","def train_evaluate_svm(data, label_encoder):\n","    X_lbp = np.array(data['lbp_features'].tolist())\n","    X_slic = np.array(data['slic_features'].tolist())\n","    X_glcm = np.array(data[['contrast', 'correlation', 'energy', 'homogeneity', 'dissimilarity']])\n","    X_histogram = np.array(data['histogram_features'].tolist())\n","\n","    X = np.concatenate((X_lbp, X_slic, X_glcm, X_histogram), axis=1)\n","\n","    y = np.array(data['class_encoded'])\n","\n","    if np.any(np.isinf(X)) or np.any(np.isnan(X)):\n","        print(\"Input data contains infinite or NaN values. Please check the data.\")\n","        return\n","\n","    if np.max(X) > 1e6:\n","        print(\"Input data contains values that are too large. Please check the data.\")\n","        return\n","\n","    # Oversample the entire dataset to balance classes\n","    over_sampler = RandomOverSampler(random_state=42)\n","    X_resampled, y_resampled = over_sampler.fit_resample(X, y)\n","\n","    scaler = StandardScaler()\n","    X_normalized = scaler.fit_transform(X_resampled)\n","\n","    # Split oversampled data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_resampled, test_size=0.2, random_state=42)\n","\n","    param_grid = {'C': [1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n","\n","    svm_classifier = SVC(probability=True)\n","\n","    with parallel_backend('threading'):\n","        grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)\n","        grid_search.fit(X_train, y_train)\n","\n","    print(\"Best parameters found by grid search:\")\n","    print(grid_search.best_params_)\n","\n","    best_svm_classifier = grid_search.best_estimator_\n","\n","    y_pred = best_svm_classifier.predict(X_test)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(\"Accuracy:\", accuracy)\n","\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","    class_report = classification_report(y_test, y_pred)\n","    print(\"Classification Report:\\n\", class_report)\n","\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","    print(\"F1 Score:\", f1)\n","\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    print(\"Precision Score:\", precision)\n","\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    print(\"Recall Score:\", recall)\n","\n","    plt.figure()\n","    plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title='Confusion matrix, without normalization')\n","    plt.show()\n","\n","    plt.figure()\n","    plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, normalize=True, title='Confusion matrix, with normalization')\n","    plt.show()\n","\n","# Main function\n","if __name__ == \"__main__\":\n","    data, label_encoder = load_data(\"E:/alll_featuresnorm.csv\")\n","\n","    train_evaluate_svm(data, label_encoder)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"UCLncLa1QxH4"},"execution_count":null,"outputs":[]}]}